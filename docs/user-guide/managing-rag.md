# Managing RAG Configurations

Retrievalâ€‘Augmented Generation (RAG) allows Stimm agents to answer questions based on your own documents. This guide explains how to create, configure, and manage RAG configurations.

## What is a RAG Configuration?

A RAG configuration defines:

- **Provider** â€“ Which vector database to use (Qdrant.Internal).
- **Collection/Index** â€“ The name of the vector collection where documents are stored.
- **Embedding model** â€“ The sentenceâ€‘transformer model used to convert text into vectors.
- **Retrieval parameters** â€“ Topâ€‘k, similarity threshold, etc.
- **Ultraâ€‘lowâ€‘latency mode** â€“ Optimizations for realâ€‘time retrieval.

Each configuration can be associated with one or more agents, enabling perâ€‘agent knowledge bases.

## Creating a RAG Configuration

### Via Web Interface

1. Navigate to **RAG** in the sidebar.
2. Click **Create RAG Configuration**.
3. Select a provider from the dropdown. The form will dynamically show the required fields for that provider.

   **Common fields:**

   - **Name** â€“ A descriptive name (e.g., â€œProductâ€‘Docsâ€, â€œInternalâ€‘Wikiâ€).
   - **Description** â€“ Optional.
   - **Collection Name** â€“ The vector collection (will be created if it doesnâ€™t exist).
   - **Embedding Model** â€“ Preâ€‘trained model from Hugging Face (e.g., `allâ€‘MiniLMâ€‘L6â€‘v2`).
   - **Topâ€‘K** â€“ Number of chunks to retrieve per query (default 5).
   - **Ultraâ€‘lowâ€‘latency** â€“ Check to enable caching and parallel retrieval.

4. Click **Save**. The configuration is now ready for document ingestion.

### Via API

```bash
curl -X POST "http://api.localhost/api/rag-configs/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Productâ€‘Docs",
    "provider": "qdrant.internal",
    "config": {
      "collection_name": "product_docs",
      "embedding_model": "allâ€‘MiniLMâ€‘L6â€‘v2",
      "top_k": 5,
      "ultra_low_latency": true
    }
  }'
```

## Uploading Documents

Once a RAG configuration exists, you can upload documents to it.

### Supported Formats

- PDF (.pdf)
- Microsoft Word (.docx)
- Markdown (.md)
- Plain text (.txt)

### Upload via Web Interface

1. Go to the RAG configuration details page (click on the configuration name).
2. Switch to the **Documents** tab.
3. Drag and drop files or click **Upload Documents**.
4. After upload, the system will automatically chunk, embed, and store the documents in the vector database. Progress is shown in the UI.

### Upload via API

```bash
curl -X POST "http://api.localhost/api/rag-configs/{config_id}/documents" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/document.pdf"
```

## Setting a Default Configuration

One RAG configuration can be marked as the default. Agents that do not have an explicit RAG configuration will use the default.

To set a configuration as default:

1. In the RAG list, click the **Set as Default** button next to the configuration.
2. Confirm the action.

You can also use the API endpoint `PUT /api/rag-configs/{id}/setâ€‘default`.

## Editing a Configuration

You can edit any field of a RAG configuration except the provider (changing the provider would require reâ€‘ingesting all documents). To edit:

1. Click the edit icon (âœï¸) next to the configuration.
2. Modify the fields and save.

## Deleting a Configuration

Deleting a RAG configuration also removes all associated documents from the vector database. This action cannot be undone.

1. Click the delete icon (ğŸ—‘) next to the configuration.
2. Confirm deletion.

## Monitoring Ingestion Status

After uploading documents, you can check ingestion status via the **Documents** tab. Each document shows its processing state (pending, processing, completed, error). Errors are logged and displayed.

## Using RAG with Agents

When creating or editing an agent, you can select a RAG configuration from the dropdown. The agent will then use that configuration for retrieval during conversations.

If you change an agentâ€™s RAG configuration, existing conversations continue with the previous configuration; new conversations will use the new one.

## Performance Tuning

- **Embedding model**: Smaller models are faster but less accurate. Choose based on your latency/accuracy tradeâ€‘off.
- **Topâ€‘K**: Higher values increase recall but also latency.
- **Ultraâ€‘lowâ€‘latency**: Enable if you need subâ€‘second retrieval. This preâ€‘loads the embedding model and caches frequent queries.
- **Chunk size**: The system uses a default chunk size of 512 tokens. This is not configurable via the UI but can be adjusted by modifying the ingestion code.

## Troubleshooting

- **â€œCollection not foundâ€**: Ensure the collection name matches exactly what the provider expects.
- **â€œEmbedding model download failedâ€**: Check network connectivity and Hugging Face token (if required).
- **â€œNo results retrievedâ€**: Verify that documents have been successfully ingested and that the query is relevant.

## Next Steps

- Learn about [Managing Agents](managingâ€‘agents.md).
- Explore the [Architecture](../developerâ€‘guide/architectureâ€‘overview.md) to understand how RAG fits into the pipeline.
- Read the [API Reference](../api/rest.md) for programmatic management.
